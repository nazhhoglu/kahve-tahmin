# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/122VTERKoqCks7nJuPULZCq4pEFZFEecM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming your data is in a file named 'your_data.csv'
df = pd.read_csv('updated_coffee_consumption.csv')

df.head()

df.dtypes

df = df.drop("Country", axis=1)

df.head()

corr_matrix = df.corr(numeric_only=True)

# Isı haritasını çizdir
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Korelasyon Matrisi")
plt.show()

df = df.drop(["Population_Millions", "Coffee_Price_USD_kg", "Year", "year_trend"], axis=1)

df.info()

df.dtypes

from sklearn.model_selection import train_test_split

# Özellikler ve hedef değişken
X = df.drop("Coffee_Consumption_kg", axis=1)
y = df["Coffee_Consumption_kg"]

# Eğitim ve test kümelerine ayırma (%80 eğitim, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Eğitim kümesi boyutu:", X_train.shape)
print("Test kümesi boyutu:", X_test.shape)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Modeli oluştur ve eğit
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Tahmin yap
y_pred = lr_model.predict(X_test)

# Performans ölçümleri
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("RMSE:", rmse)
print("R²:", r2)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Random Forest modelini oluştur
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Modeli eğitim verisiyle eğit
rf_model.fit(X_train, y_train)

# Test seti üzerinde tahmin yap
y_pred = rf_model.predict(X_test)

# Değerlendirme metrikleri
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

corr_matrix = df.corr(numeric_only=True)

# Isı haritasını çizdir
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Korelasyon Matrisi")
plt.show()

df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Veri setini yükle
data = pd.read_csv('updated_coffee_consumption.csv')
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Veri setini yükle
data = pd.read_csv('updated_coffee_consumption.csv')  # Veri setinin yolu

# Bağımsız değişkenler (features)
X = data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha',
          'Year_diff', 'log_population', 'standardized_price', 'price_population_interaction',
          'americano_ratio', 'yearly_trend']]

# Hedef değişken (target)
y = data['Coffee_Consumption_kg']

# Eğitim ve test setlerine ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest modelini oluştur
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Modeli eğitim verisiyle eğit
rf_model.fit(X_train, y_train)

# Test seti üzerinde tahmin yap
y_pred = rf_model.predict(X_test)

# Değerlendirme metrikleri
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')


# Bağımsız değişkenler (features)
X = data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha',
          'Year_diff', 'log_population', 'standardized_price', 'price_population_interaction',
          'americano_ratio', 'yearly_trend']]

# Hedef değişken (target)
y = data['Coffee_Consumption_kg']

# Eğitim ve test setlerine ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest modelini oluştur
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Modeli eğitim verisiyle eğit
rf_model.fit(X_train, y_train)

# Test seti üzerinde tahmin yap
y_pred = rf_model.predict(X_test)

# Değerlendirme metrikleri
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

!ls

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Yeni özellikleri ekleyelim
data['Cappuccino_Ratio'] = data['Coffee_Type_Cappuccino'] / (data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha']].sum(axis=1))
data['Espresso_Ratio'] = data['Coffee_Type_Espresso'] / (data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha']].sum(axis=1))
data['Latte_Ratio'] = data['Coffee_Type_Latte'] / (data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha']].sum(axis=1))
data['Mocha_Ratio'] = data['Coffee_Type_Mocha'] / (data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha']].sum(axis=1))

# Yeni etkileşim özellikleri
data['population_time_interaction'] = data['log_population'] * data['Year_diff']
data['price_population_yearly_interaction'] = data['standardized_price'] * data['log_population'] * data['yearly_trend']

# Bağımsız değişkenler (features)
X = data[['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso', 'Coffee_Type_Latte', 'Coffee_Type_Mocha',
          'Year_diff', 'log_population', 'standardized_price', 'price_population_interaction',
          'americano_ratio', 'yearly_trend', 'Cappuccino_Ratio', 'Espresso_Ratio', 'Latte_Ratio', 'Mocha_Ratio',
          'population_time_interaction', 'price_population_yearly_interaction']]

# Hedef değişken (target)
y = data['Coffee_Consumption_kg']

# Eğitim ve test setlerine ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest modelini oluştur
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Modeli eğitim verisiyle eğit
rf_model.fit(X_train, y_train)

# Test seti üzerinde tahmin yap
y_pred = rf_model.predict(X_test)

# Değerlendirme metrikleri
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

from google.colab import drive
drive.mount('/content/drive')

df.dtypes

df.to_csv("duzenlenmis_kahve_verisi.csv", index=False)

from google.colab import files
files.download("duzenlenmis_kahve_verisi.csv")

"""## Data loading

### Subtask:
Load the dataset "duzenlenmis_kahve_verisi.csv" into a pandas DataFrame.

**Reasoning**:
Load the dataset "duzenlenmis_kahve_verisi.csv" into a pandas DataFrame and display its first few rows and shape.
"""

import pandas as pd

try:
    df = pd.read_csv('duzenlenmis_kahve_verisi.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'duzenlenmis_kahve_verisi.csv' not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its structure, identify data types, check for missing values, and analyze the distribution of variables.

**Reasoning**:
I need to explore the dataset by examining data types, missing values, descriptive statistics, and distributions of variables.
"""

# Data Types
print("Data Types:\n", df.dtypes)

# Missing Values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
print("\nMissing Values:\n", missing_values)
print("\nMissing Value Percentage:\n", missing_percentage)

# Descriptive Statistics
print("\nDescriptive Statistics:\n", df.describe())

# Data Distribution (Histograms)
import matplotlib.pyplot as plt
numerical_cols = df.select_dtypes(include=['number']).columns
num_plots = len(numerical_cols)
fig, axes = plt.subplots(nrows=(num_plots + 2) // 3, ncols=min(3, num_plots), figsize=(15, 5 * ((num_plots + 2) // 3)))
axes = axes.ravel()  # Flatten the axes array for easier iteration

for i, col in enumerate(numerical_cols):
    df.hist(column=col, ax=axes[i])
    axes[i].set_title(col)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])  # Remove extra subplots if needed

plt.tight_layout()
plt.show()

# Categorical Feature Analysis
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
if len(categorical_cols) > 0:
    for col in categorical_cols:
      print(f"\nUnique values and counts for {col}:")
      print(df[col].value_counts())
      df[col].value_counts().plot(kind='bar', title=col)
      plt.show()
else:
    print("\nNo categorical columns found.")


# Correlation Matrix
correlation_matrix = df.corr()
print("\nCorrelation Matrix:\n", correlation_matrix)

"""## Data visualization

### Subtask:
Visualize the relationships between variables in the dataset.

**Reasoning**:
Visualize the relationships between variables using a heatmap, scatter plots, and histograms.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Heatmap of the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

# Scatter plots for moderately or strongly correlated variables
plt.figure(figsize=(15, 10))
for i in range(len(df.columns)):
    for j in range(i + 1, len(df.columns)):
        if abs(df.corr().iloc[i,j]) > 0.5:
            plt.subplot(3,4, (i*len(df.columns)+j)//3)
            sns.regplot(x=df.columns[i], y=df.columns[j], data=df)
plt.tight_layout()
plt.show()

# Histograms for each numerical feature
df.hist(figsize=(16, 12), bins=20)
plt.tight_layout()
plt.show()

"""**Reasoning**:
Fix the error in the previous code block by correcting the subplot indexing and regenerate the plots.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Heatmap of the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

# Scatter plots for moderately or strongly correlated variables
plt.figure(figsize=(15, 10))
subplot_num = 1
for i in range(len(df.columns)):
    for j in range(i + 1, len(df.columns)):
        if abs(df.corr().iloc[i,j]) > 0.5:
            plt.subplot(3, 4, subplot_num)
            sns.regplot(x=df.columns[i], y=df.columns[j], data=df)
            subplot_num += 1
plt.tight_layout()
plt.show()

# Histograms for each numerical feature
df.hist(figsize=(16, 12), bins=20)
plt.tight_layout()
plt.show()

"""## Feature engineering

### Subtask:
Feature Engineering based on Correlation Analysis and Visualizations

**Reasoning**:
Create interaction terms and polynomial features based on the correlation analysis and visualizations from the previous steps.  Transform skewed features based on their histograms. Document the changes made.
"""

import numpy as np

# Identify strongly correlated features (example threshold: 0.7)
correlation_matrix = df.corr()
strong_correlations = correlation_matrix[abs(correlation_matrix) > 0.7]

# Create interaction terms for strongly correlated numerical features
for i in range(len(strong_correlations.columns)):
    for j in range(i + 1, len(strong_correlations.columns)):
        col1 = strong_correlations.columns[i]
        col2 = strong_correlations.columns[j]
        if abs(correlation_matrix.loc[col1, col2]) > 0.7 and \
           np.issubdtype(df[col1].dtype, np.number) and \
           np.issubdtype(df[col2].dtype, np.number):
            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
            print(f"Created interaction term: {col1}_x_{col2}")

# Add polynomial features for features exhibiting non-linear relationships
# (example: squaring 'Year_diff')
df['Year_diff_squared'] = df['Year_diff'] ** 2
print("Created polynomial feature: Year_diff_squared")

# Identify and transform skewed features based on histograms
# (example: log transformation for 'Coffee_Consumption_kg' if it's skewed)
if df['Coffee_Consumption_kg'].skew() > 0.5:  # Example threshold
    df['log_Coffee_Consumption_kg'] = np.log1p(df['Coffee_Consumption_kg'])
    print("Applied log transformation to 'Coffee_Consumption_kg'")

    #Check for any negative values in the log transformed column ONLY if it was created.
    if (df['log_Coffee_Consumption_kg'] < 0).any():
        print("Negative values present in log transformed column.")

display(df.head())

"""**Reasoning**:
The previous code block failed due to a KeyError because 'log_Coffee_Consumption_kg' was not yet created in the DataFrame when the code tried to access it.  I need to fix the code to ensure the conditional log transformation is executed correctly and only if the skew is above the threshold.  I will also remove the negative value check since the column might not exist in the dataframe.


"""

import numpy as np

# Identify strongly correlated features (example threshold: 0.7)
correlation_matrix = df.corr()
strong_correlations = correlation_matrix[abs(correlation_matrix) > 0.7]

# Create interaction terms for strongly correlated numerical features
for i in range(len(strong_correlations.columns)):
    for j in range(i + 1, len(strong_correlations.columns)):
        col1 = strong_correlations.columns[i]
        col2 = strong_correlations.columns[j]
        if abs(correlation_matrix.loc[col1, col2]) > 0.7 and \
           np.issubdtype(df[col1].dtype, np.number) and \
           np.issubdtype(df[col2].dtype, np.number):
            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
            print(f"Created interaction term: {col1}_x_{col2}")

# Add polynomial features for features exhibiting non-linear relationships
# (example: squaring 'Year_diff')
df['Year_diff_squared'] = df['Year_diff'] ** 2
print("Created polynomial feature: Year_diff_squared")

# Identify and transform skewed features based on histograms
# (example: log transformation for 'Coffee_Consumption_kg' if it's skewed)
if df['Coffee_Consumption_kg'].skew() > 0.5:  # Example threshold
    df['log_Coffee_Consumption_kg'] = np.log1p(df['Coffee_Consumption_kg'])
    print("Applied log transformation to 'Coffee_Consumption_kg'")

display(df.head())

"""## Summary:

### Q&A

No questions were explicitly asked in the provided text.  However, the overall goal was to analyze the relationships between variables in a coffee consumption dataset, and to suggest improvements.  The analysis process addressed this goal through data loading, exploration, visualization, and feature engineering.

### Data Analysis Key Findings

* **No Missing Values:** The dataset contains no missing values, simplifying the preprocessing steps.
* **High Correlation between Variables:**  Several variables exhibit strong positive or negative correlations, as identified by the correlation matrix and heatmap.  The code specifically looks for correlations above 0.7.  Interaction terms were created for these highly correlated variables.
* **Feature Engineering:**  New features were engineered, including interaction terms for strongly correlated variables, a polynomial feature (square of 'Year_diff'), and a potential log transformation of 'Coffee_Consumption_kg' depending on its skewness.
* **Skewness of 'Coffee_Consumption_kg':** The code checks the skewness of the 'Coffee_Consumption_kg' column and applies a log transformation if it exceeds 0.5, suggesting potential non-normality in the original data.


### Insights or Next Steps

* **Model Building:**  Use the enhanced dataset (with engineered features) to train a predictive model.  The strong correlations and newly created interaction terms might improve the model's performance.  Consider exploring different regression models and evaluate their performance using appropriate metrics.
* **Further Feature Exploration:** Explore other potential features and transformations. For example, consider applying other transformations (e.g., Box-Cox) to skewed variables beyond 'Coffee_Consumption_kg', or creating features based on domain knowledge about coffee consumption patterns.

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Assuming 'df' is your DataFrame containing the data
# If not, replace 'df' with the actual DataFrame name

# Ensure 'log_Coffee_Consumption_kg' is created if 'Coffee_Consumption_kg' is skewed
if df['Coffee_Consumption_kg'].skew() > 0.5:
    df['log_Coffee_Consumption_kg'] = np.log1p(df['Coffee_Consumption_kg'])
    print("Applied log transformation to 'Coffee_Consumption_kg'")

# Define your features (X) and target (y)
# Include 'log_Coffee_Consumption_kg' only if it was created
features = ['Coffee_Type_Americano', 'Coffee_Type_Cappuccino', 'Coffee_Type_Espresso',
            'Coffee_Type_Latte', 'Coffee_Type_Mocha', 'Year_diff', 'log_population',
            'standardized_price', 'price_population_interaction', 'americano_ratio',
            'yearly_trend', 'Year_diff_squared']
if 'log_Coffee_Consumption_kg' in df.columns:
    features.append('log_Coffee_Consumption_kg')

X = df[features]  # Update with your actual feature columns
y = df['Coffee_Consumption_kg']  # Update with your actual target column

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Modeli
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Tahmin yap
y_pred_rf = rf_model.predict(X_test)

# Sonuçları değerlendirme
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest - MSE: {mse_rf}")
print(f"Random Forest - R^2: {r2_rf}")

import pandas as pd

# Veriyi yükle
df = pd.read_csv('duzenlenmis_kahve_verisi.csv')

# VIF hesaplaması için kodu çalıştır
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

X = df[[
    'yearly_trend',
    'log_population',
    'standardized_price',
    'price_population_interaction',
    'Year_diff',
    'Coffee_Type_Americano',
    'Coffee_Type_Cappuccino',
    'Coffee_Type_Espresso',
    'Coffee_Type_Latte',
    'Coffee_Type_Mocha'
]]

X = add_constant(X)

vif_data = pd.DataFrame()
vif_data['Feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

import pandas as pd

# Örnek veri setini oluşturma
data = {
    'Coffee_Consumption_kg': [9.253939, 9.981203, 3.312916, 2.436180, 4.637849],
    'Coffee_Type': ['Coffee_Type_Americano', 'Coffee_Type_Mocha', 'Coffee_Type_Latte', 'Coffee_Type_Espresso', 'Coffee_Type_Mocha']
}

df = pd.DataFrame(data)

# Kategorik 'Coffee_Type' sütununu one-hot encode etme
df_encoded = pd.get_dummies(df, columns=['Coffee_Type'])

# Sonuçları yazdırma
print(df_encoded)

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Sayısal verileri içeren dataframe (pd.get_dummies sonucu)
X = df.drop('Coffee_Consumption_kg', axis=1)  # Hedef değişkeni çıkarıyoruz

# Sabit terimi (constant) ekle
X = sm.add_constant(X)

# VIF hesaplama
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

# Verinin sayısal olup olmadığını kontrol et
print(X.dtypes)

# Eğer 'inf' veya 'NaN' varsa, bunları kontrol edelim
print(X.isna().sum())  # NaN kontrolü
print((X == float("inf")).sum())  # Inf kontrolü

# X veri çerçevesinin tüm türlerini kontrol et
print(X.dtypes)

# Coffee_Type sütununu dummies değişkenlerine dönüştür
X = pd.get_dummies(X, columns=['Coffee_Type'], drop_first=True)

# X veri çerçevesinin yeni türlerini kontrol et
print(X.dtypes)

# VIF hesaplaması
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

# Boolean sütunları sayısal verilere dönüştür
X = X.astype(int)

# VIF hesaplaması
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

# Düzenlenmiş veri setini CSV dosyasına kaydedin
X_train.to_csv('/content/duzenlenmis_X_train.csv', index=False)
y_train.to_csv('/content/duzenlenmis_y_train.csv', index=False)

print("Düzenlenmiş veri seti kaydedildi.")

from google.colab import files

# Dosyayı indirmek için
files.download('/content/duzenlenmis_X_train.csv')
files.download('/content/duzenlenmis_y_train.csv')

# X_train ve y_train'i birleştirelim
duzenlenmis_veri = pd.concat([X_train, y_train], axis=1)

# Düzenlenmiş veri setini tek bir CSV dosyasına kaydedelim
duzenlenmis_veri.to_csv('/content/duzenlenmis_veri.csv', index=False)

print("Düzenlenmiş veri seti tek bir dosya olarak kaydedildi.")

from google.colab import files

# Düzenlenmiş dosyayı indir
files.download('/content/duzenlenmis_veri.csv')

# Düzenlenmiş veriyi yükleyelim
import pandas as pd

df = pd.read_csv('/content/duzenlenmis_veri.csv')

# Özellikler ve hedef değişkeni ayıralım
X = df.drop(columns=['Coffee_Consumption_kg'])  # Bağımsız değişkenler (özellikler)
y = df['Coffee_Consumption_kg']  # Bağımlı değişken (hedef)

from sklearn.model_selection import train_test_split

# Veriyi eğitim ve test setlerine ayıralım
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Eğitim veri seti boyutu: {X_train.shape}")
print(f"Test veri seti boyutu: {X_test.shape}")

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Modeli oluştur ve eğit
model = LinearRegression()
model.fit(X_train, y_train)

# Tahmin yapalım
y_pred = model.predict(X_test)

# Sonuçları değerlendirelim
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.model_selection import train_test_split

# Özellikler (X) ve hedef değişken (y) tanımlanacak
X = df.drop(columns=['Coffee_Consumption_kg'])
y = df['Coffee_Consumption_kg']

# Veri setini eğitim ve test verisi olarak bölme (80% eğitim, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Random Forest modelini tanımla
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Modeli eğitim verisiyle eğit
rf_model.fit(X_train, y_train)

# Test verisiyle tahminler yap
y_pred = rf_model.predict(X_test)

# Modelin performansını değerlendirme
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")